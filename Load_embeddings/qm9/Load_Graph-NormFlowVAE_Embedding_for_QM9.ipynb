{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e74b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages']\n",
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages',\n",
      " '/torch_cuda/pigvae_all']\n",
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages',\n",
      " '/torch_cuda/pigvae_all',\n",
      " '/torch_cuda/ddpm-torch']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "root = '/'\n",
    "pprint.pprint(sys.path)\n",
    "\n",
    "import_path = root + 'pigvae_all'\n",
    "sys.path.append(import_path)\n",
    "pprint.pprint(sys.path)\n",
    "\n",
    "import_path2 = root + \"ddpm-torch\"\n",
    "sys.path.append(import_path2)\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ca0bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.dense import floyd_warshall_numpy\n",
    "\n",
    "from networkx.generators.random_graphs import *\n",
    "from networkx.generators.ego import ego_graph\n",
    "from networkx.generators.geometric import random_geometric_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b25d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_f = 36\n",
    "num_edge_f = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df7bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"vae\":True,\n",
    "    \"kld_loss_scale\":0.01,\n",
    "    \"perm_loss_scale\":0.1,\n",
    "    \"property_loss_scale\":0.5,\n",
    "    \"num_node_features\":num_node_f,\n",
    "    \"num_edge_features\":1+num_edge_f+1,\n",
    "    \"emb_dim\": 50,\n",
    "    'graph_encoder_hidden_dim': 256,\n",
    "    'graph_encoder_k_dim': 64,\n",
    "    'graph_encoder_v_dim': 64,\n",
    "    'graph_encoder_num_heads': 16,\n",
    "    'graph_encoder_ppf_hidden_dim': 512,\n",
    "    'graph_encoder_num_layers': 16,\n",
    "    'graph_decoder_hidden_dim': 256,\n",
    "    'graph_decoder_k_dim': 64,\n",
    "    'graph_decoder_v_dim': 64,\n",
    "    'graph_decoder_num_heads': 16,\n",
    "    'graph_decoder_ppf_hidden_dim': 512,\n",
    "    'graph_decoder_num_layers': 16,\n",
    "    \"graph_decoder_pos_emb_dim\": 64,\n",
    "    'property_predictor_hidden_dim': 3,\n",
    "    'num_properties': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcea199",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000e2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Linear, LayerNorm, Dropout\n",
    "from torch.nn.functional import relu, pad\n",
    "from pigvae.graph_transformer import Transformer, PositionalEncoding\n",
    "from pigvae.models import GraphAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d89188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as distrib\n",
    "import torch.distributions.transforms as transform\n",
    "\n",
    "class Flow(transform.Transform, nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        transform.Transform.__init__(self)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "    # Init all parameters\n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.data.uniform_(-0.01, 0.01)\n",
    "\n",
    "    # Hacky hash bypass\n",
    "    def __hash__(self):\n",
    "        return nn.Module.__hash__(self)\n",
    "\n",
    "class PlanarFlow(Flow):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(PlanarFlow, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.scale = nn.Parameter(torch.Tensor(1, dim))\n",
    "        self.bias = nn.Parameter(torch.Tensor(1))\n",
    "        self.init_parameters()\n",
    "\n",
    "    def _call(self, z):\n",
    "        f_z = F.linear(z, self.weight, self.bias)\n",
    "        return z + self.scale * torch.tanh(f_z)\n",
    "\n",
    "    def log_abs_det_jacobian(self, z):\n",
    "        f_z = F.linear(z, self.weight, self.bias)\n",
    "        psi = (1 - torch.tanh(f_z) ** 2) * self.weight\n",
    "        det_grad = 1 + torch.mm(psi, self.scale.t())\n",
    "        return torch.log(det_grad.abs() + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1718c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main class for normalizing flow\n",
    "class NormalizingFlow(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, blocks, flow_length, density):\n",
    "        super().__init__()\n",
    "        biject = []\n",
    "        for f in range(flow_length):\n",
    "            for b_flow in blocks:\n",
    "                biject.append(b_flow(dim))\n",
    "        self.transforms = transform.ComposeTransform(biject)\n",
    "        self.bijectors = nn.ModuleList(biject)\n",
    "        self.base_density = density\n",
    "        #self.final_density = distrib.TransformedDistribution(density, self.transforms)\n",
    "        self.log_det = []\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.log_det = []\n",
    "        # Applies series of flows\n",
    "        for b in range(len(self.bijectors)):\n",
    "            self.log_det.append(self.bijectors[b].log_abs_det_jacobian(z))\n",
    "            z = self.bijectors[b](z)\n",
    "\n",
    "        return z, self.log_det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fd3ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pigvae.models import GraphEncoder, GraphDecoder, Permuter\n",
    "from pigvae.models import BottleNeckEncoder, BottleNeckDecoder, PropertyPredictor\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, LayerNorm, Dropout\n",
    "from torch.nn.functional import relu, pad\n",
    "from pigvae.graph_transformer import Transformer, PositionalEncoding\n",
    "from pigvae.synthetic_graphs.data import DenseGraphBatch\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, LayerNorm, Dropout\n",
    "from torch.nn.functional import relu, pad\n",
    "from pigvae.graph_transformer import Transformer, PositionalEncoding\n",
    "#from pigvae.synthetic_graphs.data import DenseGraphBatch\n",
    "\n",
    "block_planar = [PlanarFlow]\n",
    "\n",
    "class GraphFlowVAE(torch.nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.vae = hparams[\"vae\"]\n",
    "        self.encoder = GraphEncoder(hparams)\n",
    "        self.bottle_neck_encoder = BottleNeckEncoder(hparams)\n",
    "        self.bottle_neck_decoder = BottleNeckDecoder(hparams)\n",
    "        self.property_predictor = PropertyPredictor(hparams)\n",
    "        self.permuter = Permuter(hparams)\n",
    "        self.decoder = GraphDecoder(hparams)\n",
    "        self.flow = NormalizingFlow(dim=hparams[\"emb_dim\"], blocks=block_planar, flow_length=16, \\\n",
    "                               density=distrib.MultivariateNormal(torch.zeros(hparams[\"emb_dim\"]), torch.eye(hparams[\"emb_dim\"])))\n",
    "\n",
    "    def encode(self, node_features, edge_features, mask):\n",
    "\n",
    "        graph_emb, node_features = self.encoder(\n",
    "            node_features=node_features,\n",
    "            edge_features=edge_features,\n",
    "            mask=mask,\n",
    "        )\n",
    "        graph_emb, mu, logvar = self.bottle_neck_encoder(graph_emb)\n",
    "\n",
    "        return graph_emb, node_features, mu, logvar\n",
    "\n",
    "    def decode(self, graph_emb, perm, mask=None):\n",
    "        props = self.property_predictor(graph_emb).squeeze()\n",
    "\n",
    "        graph_emb = self.bottle_neck_decoder(graph_emb)\n",
    "        node_logits, edge_logits = self.decoder(\n",
    "            graph_emb=graph_emb,\n",
    "            perm=perm,\n",
    "            mask=mask\n",
    "        )\n",
    "\n",
    "        return node_logits, edge_logits, props\n",
    "\n",
    "    def forward(self, node_features, edge_features, mask, training, tau):\n",
    "\n",
    "        #sampling z0 with VAE encoder\n",
    "        z_0, node_features, mu, logvar = self.encode(node_features, edge_features, mask)\n",
    "        perm = self.permuter(node_features, mask=mask, hard=not training, tau=tau)\n",
    "\n",
    "        # sampling posterior  zk with flows\n",
    "        z_k, list_ladj = self.flow(z_0)\n",
    "        #Cross entropy ln p(z_k)\n",
    "        log_p_zk = -0.5 * z_k * z_k\n",
    "        #Entropy ln q(z_0)\n",
    "        log_q_z0 = - torch.mean(torch.sum(logvar,dim=1)) * 1e-3\n",
    "        #  ln q(z_0) - ln p(z_k)\n",
    "        logs = (- log_p_zk).sum()\n",
    "        # Add log determinants\n",
    "        ladj = torch.cat(list_ladj)\n",
    "        # ln q(z_0) - ln p(z_k) - sum[log det]\n",
    "        logs -= torch.sum(ladj) #all flow loss\n",
    "\n",
    "        nflow_loss = (logs / float(n_batch)) + log_q_z0\n",
    "        graph_pred = self.decode(z_k, perm, mask)\n",
    "\n",
    "        return graph_pred, perm, z_0, z_k, list_ladj, nflow_loss, mu, logvar\n",
    "\n",
    "\n",
    "class BottleNeckEncoder(torch.nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.d_in = hparams[\"graph_encoder_hidden_dim\"]\n",
    "        self.d_out = hparams[\"emb_dim\"]\n",
    "        self.vae = hparams[\"vae\"]\n",
    "        if self.vae:\n",
    "            self.w = Linear(self.d_in, 2 * self.d_out)\n",
    "        else:\n",
    "            self.w = Linear(self.d_in, self.d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w(relu(x))\n",
    "        if self.vae:\n",
    "            mu = x[:, :self.d_out]\n",
    "            logvar = x[:, self.d_out:]\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            x = mu + eps * std\n",
    "            return x, mu, logvar\n",
    "        else:\n",
    "            return x, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72bd9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphFlowVAE(hparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f0ffb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFlowVAE(\n",
       "  (encoder): GraphEncoder(\n",
       "    (posiotional_embedding): PositionalEncoding()\n",
       "    (graph_transformer): Transformer(\n",
       "      (self_attn_layers): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pff_layers): ModuleList(\n",
       "        (0): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (12): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (13): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (14): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (15): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_in): Linear(in_features=83, out_features=256, bias=True)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (bottle_neck_encoder): BottleNeckEncoder(\n",
       "    (w): Linear(in_features=256, out_features=100, bias=True)\n",
       "  )\n",
       "  (bottle_neck_decoder): BottleNeckDecoder(\n",
       "    (w): Linear(in_features=50, out_features=256, bias=True)\n",
       "  )\n",
       "  (property_predictor): PropertyPredictor(\n",
       "    (w_1): Linear(in_features=50, out_features=3, bias=True)\n",
       "    (w_2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (w_3): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (layer_norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (permuter): Permuter(\n",
       "    (scoring_fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): GraphDecoder(\n",
       "    (posiotional_embedding): PositionalEncoding()\n",
       "    (graph_transformer): Transformer(\n",
       "      (self_attn_layers): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pff_layers): ModuleList(\n",
       "        (0): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (12): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (13): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (14): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (15): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_in): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (node_fc_out): Linear(in_features=256, out_features=36, bias=True)\n",
       "    (edge_fc_out): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (flow): NormalizingFlow(\n",
       "    (bijectors): ModuleList(\n",
       "      (0): PlanarFlow()\n",
       "      (1): PlanarFlow()\n",
       "      (2): PlanarFlow()\n",
       "      (3): PlanarFlow()\n",
       "      (4): PlanarFlow()\n",
       "      (5): PlanarFlow()\n",
       "      (6): PlanarFlow()\n",
       "      (7): PlanarFlow()\n",
       "      (8): PlanarFlow()\n",
       "      (9): PlanarFlow()\n",
       "      (10): PlanarFlow()\n",
       "      (11): PlanarFlow()\n",
       "      (12): PlanarFlow()\n",
       "      (13): PlanarFlow()\n",
       "      (14): PlanarFlow()\n",
       "      (15): PlanarFlow()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_dir = root + 'save_models/qm9/pig-beta-e3nflow-vae_models/'\n",
    "\n",
    "model = torch.load(load_model_dir + \"pigvae_best_model.pt\")\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d667d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "112ca6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import yaml\n",
    "#from easydict import EasyDict\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torch\n",
    "#import torch.utils.tensorboard\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edd37e",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0cb3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickes(load_dir):\n",
    "    with open(load_dir + \"node_features.pickle\", 'br') as fa:\n",
    "        node_features = pickle.load(fa)\n",
    "        fa.close\n",
    "          \n",
    "    with open(load_dir + \"edge_features.pickle\", 'br') as fb:\n",
    "        edge_features = pickle.load(fb)\n",
    "        fb.close\n",
    "          \n",
    "    with open(load_dir + \"masks.pickle\", 'br') as fc:\n",
    "        masks = pickle.load(fc)\n",
    "        fc.close\n",
    "\n",
    "    with open(load_dir + \"props.pickle\", 'br') as fd:\n",
    "        props = pickle.load(fd)\n",
    "        fd.close\n",
    "\n",
    "    with open(load_dir + \"all_targets.pickle\", 'br') as fe:\n",
    "        targets = pickle.load(fe)\n",
    "        fe.close\n",
    "        \n",
    "    return node_features, edge_features, masks, props, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e9b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_to_embs(dataloader):\n",
    "    \n",
    "    z_mus = []\n",
    "    z_vars= []\n",
    "    zk_list = []\n",
    "    t_list = []\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(dataloader):\n",
    "\n",
    "        batch_stds = []\n",
    "        \n",
    "        if batch_idx % 10 == 0 :\n",
    "            print(batch_idx)\n",
    "\n",
    "        node_features, edge_features, mask, props, targets = batch_data\n",
    "        node_features, edge_features, mask, props = node_features.to(device), edge_features.to(device), mask.to(device), props.to(device)\n",
    "    \n",
    "        z_0, node_features, mu, logvar = model.encode(node_features, edge_features, mask)\n",
    "        z_k, list_ladj = model.flow(mu)\n",
    "        batch_std = torch.exp(0.5 * logvar)\n",
    "    \n",
    "        for std in batch_std:\n",
    "            diag_matrix = torch.diag(std * std)\n",
    "            batch_stds.append(diag_matrix.cpu().detach().numpy())\n",
    "        \n",
    "        batch_stds = np.reshape(np.array(batch_stds), (-1, hparams[\"emb_dim\"], hparams[\"emb_dim\"]))\n",
    "\n",
    "        z_vars.extend(batch_stds)\n",
    "        z_mus.extend(mu.cpu().detach().numpy())\n",
    "        zk_list.extend(z_k.cpu().detach().numpy())\n",
    "        t_list.extend(targets.cpu().detach().numpy())\n",
    "    \n",
    "        del mu, logvar, batch_std, z_k\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return np.array(z_mus), np.array(zk_list), np.array(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38983f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/torch_cuda/save_qm9-models/pig-beta-e3nflow-vae_models_50d/samples_for_mcmc/\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "2560\n",
      "2570\n",
      "2580\n",
      "2590\n",
      "2600\n",
      "2610\n",
      "2620\n",
      "2630\n",
      "2640\n",
      "2650\n",
      "2660\n",
      "2670\n",
      "2680\n",
      "2690\n",
      "2700\n",
      "2710\n",
      "2720\n",
      "2730\n",
      "2740\n",
      "2750\n",
      "2760\n",
      "2770\n",
      "2780\n",
      "2790\n",
      "2800\n",
      "2810\n",
      "2820\n",
      "2830\n",
      "2840\n",
      "2850\n",
      "2860\n",
      "2870\n",
      "2880\n",
      "2890\n",
      "2900\n",
      "2910\n",
      "2920\n",
      "2930\n",
      "2940\n",
      "2950\n",
      "2960\n",
      "2970\n",
      "2980\n",
      "2990\n",
      "3000\n",
      "3010\n",
      "3020\n",
      "3030\n",
      "3040\n",
      "3050\n",
      "3060\n",
      "3070\n",
      "3080\n",
      "3090\n",
      "3100\n",
      "3110\n",
      "3120\n",
      "3130\n",
      "3140\n",
      "3150\n",
      "3160\n",
      "3170\n",
      "3180\n",
      "3190\n",
      "3200\n",
      "3210\n",
      "3220\n",
      "3230\n",
      "3240\n",
      "3250\n",
      "3260\n",
      "3270\n",
      "3280\n",
      "3290\n",
      "3300\n",
      "3310\n",
      "3320\n",
      "3330\n",
      "3340\n",
      "3350\n",
      "3360\n",
      "3370\n",
      "3380\n",
      "3390\n",
      "3400\n",
      "3410\n",
      "3420\n",
      "3430\n",
      "3440\n",
      "3450\n",
      "3460\n",
      "3470\n",
      "3480\n",
      "3490\n",
      "3500\n",
      "3510\n",
      "3520\n",
      "3530\n",
      "3540\n",
      "3550\n",
      "3560\n",
      "3570\n",
      "3580\n",
      "3590\n",
      "3600\n",
      "3610\n",
      "3620\n",
      "3630\n",
      "3640\n",
      "3650\n",
      "3660\n",
      "3670\n",
      "3680\n",
      "3690\n",
      "3700\n",
      "3710\n",
      "3720\n",
      "3730\n",
      "3740\n",
      "3750\n",
      "3760\n",
      "3770\n",
      "3780\n",
      "3790\n",
      "3800\n",
      "3810\n",
      "3820\n",
      "3830\n",
      "3840\n",
      "3850\n",
      "3860\n",
      "3870\n",
      "3880\n",
      "3890\n",
      "3900\n",
      "3910\n",
      "3920\n",
      "3930\n",
      "3940\n",
      "3950\n",
      "3960\n",
      "3970\n",
      "3980\n",
      "3990\n",
      "4000\n",
      "4010\n",
      "4020\n",
      "4030\n",
      "4040\n",
      "4050\n",
      "4060\n",
      "4070\n",
      "4080\n",
      "4090\n",
      "4100\n",
      "4110\n",
      "4120\n",
      "4130\n",
      "4140\n",
      "4150\n",
      "4160\n",
      "4170\n",
      "4180\n",
      "4190\n",
      "4200\n",
      "4210\n",
      "4220\n",
      "4230\n",
      "4240\n",
      "4250\n",
      "4260\n",
      "4270\n",
      "4280\n",
      "4290\n",
      "4300\n",
      "4310\n",
      "4320\n",
      "4330\n",
      "4340\n",
      "4350\n",
      "4360\n",
      "4370\n",
      "4380\n",
      "4390\n",
      "4400\n",
      "4410\n",
      "4420\n",
      "4430\n",
      "4440\n",
      "4450\n",
      "4460\n",
      "4470\n",
      "4480\n",
      "4490\n",
      "4500\n",
      "4510\n",
      "4520\n",
      "4530\n",
      "4540\n",
      "4550\n",
      "4560\n",
      "4570\n",
      "4580\n",
      "4590\n",
      "4600\n",
      "4610\n",
      "4620\n",
      "4630\n",
      "4640\n",
      "4650\n",
      "4660\n",
      "4670\n",
      "4680\n",
      "4690\n",
      "4700\n",
      "4710\n",
      "4720\n",
      "4730\n",
      "4740\n",
      "4750\n",
      "4760\n",
      "4770\n",
      "4780\n",
      "4790\n",
      "4800\n",
      "4810\n",
      "4820\n",
      "4830\n",
      "4840\n",
      "4850\n",
      "4860\n",
      "4870\n",
      "4880\n",
      "4890\n",
      "4900\n",
      "4910\n",
      "4920\n",
      "4930\n",
      "4940\n",
      "4950\n",
      "4960\n",
      "4970\n",
      "4980\n",
      "4990\n",
      "(100000, 50) (100000, 8)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "load_dir = root + \"dataset/eval_dataset/molecular_properties/qm9/e3graphs/\"\n",
    "sample_save_dir = load_model_dir + \"samples_for_mcmc/\"\n",
    "\n",
    "print(sample_save_dir)\n",
    "\n",
    "if not os.path.exists(sample_save_dir):\n",
    "    os.makedirs(sample_save_dir)\n",
    "\n",
    "node_features, edge_features, masks, props, targets = load_pickes(load_dir)\n",
    "dataset = torch.utils.data.TensorDataset(node_features, edge_features, masks, props, targets)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "z_mus, z_ks, ys = graphs_to_embs(dataloader)\n",
    "print(z_mus.shape, ys.shape)\n",
    "\n",
    "np.save(sample_save_dir + \"embs_mu.npy\", z_mus)\n",
    "np.save(sample_save_dir + \"embs_zk.npy\", z_ks)\n",
    "np.save(sample_save_dir + \"all_targets.npy\", ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a57e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
