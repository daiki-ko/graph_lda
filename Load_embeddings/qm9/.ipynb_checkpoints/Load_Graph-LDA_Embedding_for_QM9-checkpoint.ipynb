{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e74b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages']\n",
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages',\n",
      " '/torch_cuda/pigvae_all']\n",
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages',\n",
      " '/torch_cuda/pigvae_all',\n",
      " '/torch_cuda/ddpm-torch']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "root = '/'\n",
    "pprint.pprint(sys.path)\n",
    "\n",
    "import_path = root + 'pigvae_all'\n",
    "sys.path.append(import_path)\n",
    "pprint.pprint(sys.path)\n",
    "\n",
    "import_path2 = root + \"ddpm-torch\"\n",
    "sys.path.append(import_path2)\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ca0bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.dense import floyd_warshall_numpy\n",
    "\n",
    "from networkx.generators.random_graphs import *\n",
    "from networkx.generators.ego import ego_graph\n",
    "from networkx.generators.geometric import random_geometric_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b25d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_f = 36\n",
    "num_edge_f = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df7bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"vae\":True,\n",
    "    \"kld_loss_scale\":0.01,\n",
    "    \"perm_loss_scale\":0.1,\n",
    "    \"property_loss_scale\":0.5,\n",
    "    \"num_node_features\":num_node_f,\n",
    "    \"num_edge_features\":1+num_edge_f+1,\n",
    "    \"emb_dim\": 50,\n",
    "    'graph_encoder_hidden_dim': 256,\n",
    "    'graph_encoder_k_dim': 64,\n",
    "    'graph_encoder_v_dim': 64,\n",
    "    'graph_encoder_num_heads': 16,\n",
    "    'graph_encoder_ppf_hidden_dim': 512,\n",
    "    'graph_encoder_num_layers': 16,\n",
    "    'graph_decoder_hidden_dim': 256,\n",
    "    'graph_decoder_k_dim': 64,\n",
    "    'graph_decoder_v_dim': 64,\n",
    "    'graph_decoder_num_heads': 16,\n",
    "    'graph_decoder_ppf_hidden_dim': 512,\n",
    "    'graph_decoder_num_layers': 16,\n",
    "    \"graph_decoder_pos_emb_dim\": 64,\n",
    "    'property_predictor_hidden_dim': 3,\n",
    "    'num_properties': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcea199",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000e2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Linear, LayerNorm, Dropout\n",
    "from torch.nn.functional import relu, pad\n",
    "from pigvae.graph_transformer import Transformer, PositionalEncoding\n",
    "#from pigvae.synthetic_graphs.data import DenseGraphBatch\n",
    "\n",
    "from pigvae.models import GraphEncoder, GraphDecoder, Permuter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e4a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from ddpm_torch.utils import seed_all, infer_range\n",
    "from ddpm_torch.toy import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ddpm_torch.modules import Linear, Sequential\n",
    "from ddpm_torch.functions import get_timestep_embedding\n",
    "\n",
    "\n",
    "DEFAULT_NORMALIZER = nn.LayerNorm\n",
    "DEFAULT_NONLINEARITY = nn.LeakyReLU(negative_slope=0.02, inplace=True)\n",
    "\n",
    "\n",
    "class TemporalLayer(nn.Module):\n",
    "    normalize = DEFAULT_NORMALIZER\n",
    "    nonlinearity = DEFAULT_NONLINEARITY\n",
    "\n",
    "    def __init__(self, in_features, out_features, temporal_features):\n",
    "        super(TemporalLayer, self).__init__()\n",
    "        self.norm1 = self.normalize(in_features)\n",
    "        self.fc1 = Linear(in_features, out_features, bias=False)\n",
    "        self.norm2 = self.normalize(out_features)\n",
    "        self.fc2 = Linear(out_features, out_features, bias=False)\n",
    "        self.enc = Linear(temporal_features, out_features)\n",
    "\n",
    "        self.skip = nn.Identity() if in_features == out_features else Linear(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = self.fc1(self.nonlinearity(self.norm1(x)))\n",
    "        out += self.enc(t_emb)\n",
    "        out = self.fc2(self.nonlinearity(self.norm2(out)))\n",
    "        skip = self.skip(x)\n",
    "        return out + skip\n",
    "\n",
    "\n",
    "class Denoiser(nn.Module):\n",
    "    normalize = DEFAULT_NORMALIZER\n",
    "    nonlinearity = DEFAULT_NONLINEARITY\n",
    "\n",
    "    def __init__(self, in_features, mid_features, num_temporal_layers):\n",
    "        super(Denoiser, self).__init__()\n",
    "\n",
    "        self.in_fc = Linear(in_features, mid_features, bias=False)\n",
    "        self.temp_fc = Sequential(*([TemporalLayer(\n",
    "            mid_features, mid_features, mid_features), ] * num_temporal_layers))\n",
    "        self.out_norm = self.normalize(mid_features)\n",
    "        self.out_fc = Linear(mid_features, in_features)\n",
    "        self.t_proj = nn.Sequential(\n",
    "            Linear(mid_features, mid_features),\n",
    "            self.nonlinearity)\n",
    "        self.mid_features = mid_features\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = get_timestep_embedding(t, self.mid_features)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        out = self.in_fc(x)\n",
    "        out = self.temp_fc(out, t_emb=t_emb)\n",
    "        out = self.out_fc(self.out_norm(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24aed424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "model_mean_type = \"eps\"\n",
    "model_var_type = \"fixed-large\"\n",
    "loss_type = \"mse\"\n",
    "lat_dim = 50\n",
    "in_features = lat_dim\n",
    "out_features = 2 * in_features if model_var_type == \"learned\" else in_features\n",
    "mid_features = 256\n",
    "num_temporal_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb7cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion parameters\n",
    "beta_schedule = \"linear\"\n",
    "beta_start, beta_end = 0.001, 0.2\n",
    "timesteps = 100\n",
    "betas = get_beta_schedule(beta_schedule, beta_start=beta_start, beta_end=beta_end, timesteps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3c5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pigvae.models import GraphEncoder, GraphDecoder, Permuter\n",
    "from pigvae.models import BottleNeckEncoder, BottleNeckDecoder, PropertyPredictor\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, LayerNorm, Dropout\n",
    "from torch.nn.functional import relu, pad\n",
    "from pigvae.graph_transformer import Transformer, PositionalEncoding\n",
    "from pigvae.synthetic_graphs.data import DenseGraphBatch\n",
    "\n",
    "\n",
    "class GraphLDA(torch.nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        #self.vae = hparams[\"vae\"]\n",
    "        self.encoder = GraphEncoder(hparams)\n",
    "        self.bottle_neck_encoder = BottleNeckEncoder(hparams)\n",
    "        self.bottle_neck_decoder = BottleNeckDecoder(hparams)\n",
    "        self.property_predictor = PropertyPredictor(hparams)\n",
    "        self.permuter = Permuter(hparams)\n",
    "        self.decoder = GraphDecoder(hparams)\n",
    "\n",
    "        self.dense_fn = Denoiser(in_features, mid_features, num_temporal_layers)\n",
    "        self.diffusion = GaussianDiffusion(betas=betas, model_mean_type=model_mean_type, model_var_type=model_var_type, loss_type=loss_type)\n",
    "\n",
    "    def encode(self, node_features, edge_features, mask):\n",
    "\n",
    "        graph_emb, node_features = self.encoder(\n",
    "            node_features=node_features,\n",
    "            edge_features=edge_features,\n",
    "            mask=mask,\n",
    "        )\n",
    "        graph_emb, mu, logvar = self.bottle_neck_encoder(graph_emb)\n",
    "        return  graph_emb, mu, logvar, node_features\n",
    "\n",
    "    def decode(self, graph_emb, perm, mask=None):\n",
    "        props = self.property_predictor(graph_emb).squeeze()\n",
    "\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            num_nodes = torch.round(props * STD_NUM_NODES + MEAN_NUM_NODES).long()\n",
    "            mask = torch.arange(max(num_nodes)).type_as(num_nodes).unsqueeze(0) < num_nodes.unsqueeze(1)\n",
    "        \"\"\"\n",
    "\n",
    "        graph_emb = self.bottle_neck_decoder(graph_emb)\n",
    "        node_logits, edge_logits = self.decoder(\n",
    "            graph_emb=graph_emb,\n",
    "            perm=perm,\n",
    "            mask=mask\n",
    "        )\n",
    "\n",
    "        return node_logits, edge_logits, props\n",
    "\n",
    "    def forward(self, node_features, edge_features, mask, training, tau):\n",
    "        graph_emb, mu, logvar, node_features = self.encode(node_features, edge_features, mask)\n",
    "        perm = self.permuter(node_features, mask=mask, hard=not training, tau=tau)\n",
    "\n",
    "        #diffusion process\n",
    "        z_0 = graph_emb\n",
    "        B = z_0.shape[0]\n",
    "        T = self.diffusion.timesteps\n",
    "        t = torch.randint(T, size=(B, ), dtype=torch.int64, device=device)\n",
    "        t_noise = torch.randn_like(graph_emb)\n",
    "        z_t = self.diffusion.q_sample(z_0, t, noise=t_noise)\n",
    "\n",
    "        #denoising\n",
    "        model_out = self.dense_fn(z_t, t) #model_out : 出力, t_noise : 予測するターゲット\n",
    "\n",
    "        #deocde from noisy latent vector\n",
    "        graph_pred = self.decode(z_0, perm, mask)\n",
    "\n",
    "        return graph_pred, perm, graph_emb, mu, logvar, model_out, t_noise\n",
    "\n",
    "class BottleNeckEncoder(torch.nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.d_in = hparams[\"graph_encoder_hidden_dim\"]\n",
    "        self.d_out = hparams[\"emb_dim\"]\n",
    "        self.w = Linear(self.d_in, 2 * self.d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.w(relu(x))\n",
    "\n",
    "        mu = z[:, :self.d_out]\n",
    "        logvar = z[:, self.d_out:]\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f562e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphLDA(hparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f0ffb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphLDA(\n",
       "  (encoder): GraphEncoder(\n",
       "    (posiotional_embedding): PositionalEncoding()\n",
       "    (graph_transformer): Transformer(\n",
       "      (self_attn_layers): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pff_layers): ModuleList(\n",
       "        (0): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (12): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (13): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (14): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (15): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_in): Linear(in_features=83, out_features=256, bias=True)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (bottle_neck_encoder): BottleNeckEncoder(\n",
       "    (w): Linear(in_features=256, out_features=100, bias=True)\n",
       "  )\n",
       "  (bottle_neck_decoder): BottleNeckDecoder(\n",
       "    (w): Linear(in_features=50, out_features=256, bias=True)\n",
       "  )\n",
       "  (property_predictor): PropertyPredictor(\n",
       "    (w_1): Linear(in_features=50, out_features=3, bias=True)\n",
       "    (w_2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (w_3): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (layer_norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (permuter): Permuter(\n",
       "    (scoring_fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): GraphDecoder(\n",
       "    (posiotional_embedding): PositionalEncoding()\n",
       "    (graph_transformer): Transformer(\n",
       "      (self_attn_layers): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pff_layers): ModuleList(\n",
       "        (0): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (12): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (13): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (14): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (15): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_in): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (node_fc_out): Linear(in_features=256, out_features=36, bias=True)\n",
       "    (edge_fc_out): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dense_fn): Denoiser(\n",
       "    (in_fc): Linear(in_features=50, out_features=256, bias=False)\n",
       "    (temp_fc): Sequential(\n",
       "      (0): TemporalLayer(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (enc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (skip): Identity()\n",
       "      )\n",
       "      (1): TemporalLayer(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (enc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (skip): Identity()\n",
       "      )\n",
       "      (2): TemporalLayer(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (enc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (skip): Identity()\n",
       "      )\n",
       "    )\n",
       "    (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (out_fc): Linear(in_features=256, out_features=50, bias=True)\n",
       "    (t_proj): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.02, inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_dir = root + 'save_models/qm9/pig-beta-e3diffvae_models/'\n",
    "\n",
    "model = torch.load(load_model_dir + \"pigvae_best_model.pt\")\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523c96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "112ca6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import yaml\n",
    "#from easydict import EasyDict\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torch\n",
    "#import torch.utils.tensorboard\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edd37e",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0cb3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickes(load_dir):\n",
    "    with open(load_dir + \"node_features.pickle\", 'br') as fa:\n",
    "        node_features = pickle.load(fa)\n",
    "        fa.close\n",
    "          \n",
    "    with open(load_dir + \"edge_features.pickle\", 'br') as fb:\n",
    "        edge_features = pickle.load(fb)\n",
    "        fb.close\n",
    "          \n",
    "    with open(load_dir + \"masks.pickle\", 'br') as fc:\n",
    "        masks = pickle.load(fc)\n",
    "        fc.close\n",
    "\n",
    "    with open(load_dir + \"props.pickle\", 'br') as fd:\n",
    "        props = pickle.load(fd)\n",
    "        fd.close\n",
    "\n",
    "    with open(load_dir + \"all_targets.pickle\", 'br') as fe:\n",
    "        targets = pickle.load(fe)\n",
    "        fe.close\n",
    "        \n",
    "    return node_features, edge_features, masks, props, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5e9b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_to_embs(dataloader):\n",
    "    \n",
    "    z_mus = []\n",
    "    z_vars= []\n",
    "    t_list = []\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(dataloader):\n",
    "\n",
    "        batch_stds = []\n",
    "\n",
    "        node_features, edge_features, mask, props, targets = batch_data\n",
    "        node_features, edge_features, mask, props = node_features.to(device), edge_features.to(device), mask.to(device), props.to(device)\n",
    "\n",
    "        z, mu, logvar, node_features = model.encode(node_features, edge_features, mask)\n",
    "        batch_std = torch.exp(0.5 * logvar)\n",
    "    \n",
    "        for std in batch_std:\n",
    "            diag_matrix = torch.diag(std * std)\n",
    "            batch_stds.append(diag_matrix.cpu().detach().numpy())\n",
    "        \n",
    "        batch_stds = np.reshape(np.array(batch_stds), (-1, hparams[\"emb_dim\"], hparams[\"emb_dim\"]))\n",
    "\n",
    "        z_vars.extend(batch_stds)\n",
    "        z_mus.extend(mu.cpu().detach().numpy())\n",
    "        \n",
    "        t_list.extend(targets.cpu().detach().numpy())\n",
    "    \n",
    "        del z, mu, logvar, batch_std\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(batch_idx)\n",
    "        \n",
    "    return np.array(z_mus), np.array(z_vars), np.array(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605717a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38983f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/torch_cuda/save_qm9-models/pig-beta-weighted-e3diffvae_models_50d/samples_for_mcmc/\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "(100000, 50) (100000, 50, 50) (100000, 8)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "load_dir = root + \"dataset/eval_dataset/molecular_properties/qm9/e3graphs/\"\n",
    "sample_save_dir = load_model_dir + \"samples_for_mcmc/\"\n",
    "\n",
    "print(sample_save_dir)\n",
    "\n",
    "if not os.path.exists(sample_save_dir):\n",
    "    os.makedirs(sample_save_dir)\n",
    "\n",
    "node_features, edge_features, masks, props, targets = load_pickes(load_dir)\n",
    "dataset = torch.utils.data.TensorDataset(node_features, edge_features, masks, props, targets)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "z_mus, z_vars, ys = graphs_to_embs(dataloader)\n",
    "print(z_mus.shape, z_vars.shape, ys.shape)\n",
    "\n",
    "np.save(sample_save_dir + \"embs_mu.npy\", z_mus)\n",
    "np.save(sample_save_dir + \"embs_var.npy\", z_vars)\n",
    "np.save(sample_save_dir + \"all_targets.npy\", ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "839a57e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "root = '/torch_cuda/'\n",
    "load_model_dir = root + 'save_qm9-models/pig-beta-weighted-e3diffvae_models_50d/'\n",
    "val_vae_loss = np.load(load_model_dir + \"val_loss_hist.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc98c70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f65a9b25510>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 921.6x691.2 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "base_axis = []\n",
    "\n",
    "for id in range(len(val_vae_loss)):\n",
    "    base_axis.append(id)\n",
    "\n",
    "base_axis = np.array(base_axis)\n",
    "\n",
    "# 折れ線グラフを出力\n",
    "val_vae_loss = np.array(val_vae_loss)\n",
    "plt.plot(base_axis, val_vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28f11f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.543267485951684 78\n"
     ]
    }
   ],
   "source": [
    "print(np.min(val_vae_loss), np.argmin(val_vae_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5ac54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
