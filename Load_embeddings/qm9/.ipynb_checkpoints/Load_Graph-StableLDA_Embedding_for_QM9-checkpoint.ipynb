{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e74b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages']\n",
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages',\n",
      " '/torch_cuda/pigvae_all']\n",
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages',\n",
      " '/torch_cuda/pigvae_all',\n",
      " '/torch_cuda/ddpm-torch']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "root = '/'\n",
    "pprint.pprint(sys.path)\n",
    "\n",
    "import_path = root + 'pigvae_all'\n",
    "sys.path.append(import_path)\n",
    "pprint.pprint(sys.path)\n",
    "\n",
    "import_path2 = root + \"ddpm-torch\"\n",
    "sys.path.append(import_path2)\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ca0bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.dense import floyd_warshall_numpy\n",
    "\n",
    "from networkx.generators.random_graphs import *\n",
    "from networkx.generators.ego import ego_graph\n",
    "from networkx.generators.geometric import random_geometric_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b25d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_f = 36\n",
    "num_edge_f = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df7bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"vae\":True,\n",
    "    \"kld_loss_scale\":0.01,\n",
    "    \"perm_loss_scale\":0.1,\n",
    "    \"property_loss_scale\":0.5,\n",
    "    \"num_node_features\":num_node_f,\n",
    "    \"num_edge_features\":1+num_edge_f+1,\n",
    "    \"emb_dim\": 50,\n",
    "    'graph_encoder_hidden_dim': 256,\n",
    "    'graph_encoder_k_dim': 64,\n",
    "    'graph_encoder_v_dim': 64,\n",
    "    'graph_encoder_num_heads': 16,\n",
    "    'graph_encoder_ppf_hidden_dim': 512,\n",
    "    'graph_encoder_num_layers': 16,\n",
    "    'graph_decoder_hidden_dim': 256,\n",
    "    'graph_decoder_k_dim': 64,\n",
    "    'graph_decoder_v_dim': 64,\n",
    "    'graph_decoder_num_heads': 16,\n",
    "    'graph_decoder_ppf_hidden_dim': 512,\n",
    "    'graph_decoder_num_layers': 16,\n",
    "    \"graph_decoder_pos_emb_dim\": 64,\n",
    "    'property_predictor_hidden_dim': 3,\n",
    "    'num_properties': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcea199",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000e2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Linear, LayerNorm, Dropout\n",
    "from torch.nn.functional import relu, pad\n",
    "from pigvae.graph_transformer import Transformer, PositionalEncoding\n",
    "#from pigvae.synthetic_graphs.data import DenseGraphBatch\n",
    "\n",
    "from pigvae.models import GraphEncoder, GraphDecoder, Permuter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e4a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from ddpm_torch.utils import seed_all, infer_range\n",
    "from ddpm_torch.toy import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ddpm_torch.modules import Linear, Sequential\n",
    "from ddpm_torch.functions import get_timestep_embedding\n",
    "\n",
    "\n",
    "DEFAULT_NORMALIZER = nn.LayerNorm\n",
    "DEFAULT_NONLINEARITY = nn.LeakyReLU(negative_slope=0.02, inplace=True)\n",
    "\n",
    "\n",
    "class TemporalLayer(nn.Module):\n",
    "    normalize = DEFAULT_NORMALIZER\n",
    "    nonlinearity = DEFAULT_NONLINEARITY\n",
    "\n",
    "    def __init__(self, in_features, out_features, temporal_features):\n",
    "        super(TemporalLayer, self).__init__()\n",
    "        self.norm1 = self.normalize(in_features)\n",
    "        self.fc1 = Linear(in_features, out_features, bias=False)\n",
    "        self.norm2 = self.normalize(out_features)\n",
    "        self.fc2 = Linear(out_features, out_features, bias=False)\n",
    "        self.enc = Linear(temporal_features, out_features)\n",
    "\n",
    "        self.skip = nn.Identity() if in_features == out_features else Linear(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        out = self.fc1(self.nonlinearity(self.norm1(x)))\n",
    "        out += self.enc(t_emb)\n",
    "        out = self.fc2(self.nonlinearity(self.norm2(out)))\n",
    "        skip = self.skip(x)\n",
    "        return out + skip\n",
    "\n",
    "\n",
    "class Denoiser(nn.Module):\n",
    "    normalize = DEFAULT_NORMALIZER\n",
    "    nonlinearity = DEFAULT_NONLINEARITY\n",
    "\n",
    "    def __init__(self, in_features, mid_features, num_temporal_layers):\n",
    "        super(Denoiser, self).__init__()\n",
    "\n",
    "        self.in_fc = Linear(in_features, mid_features, bias=False)\n",
    "        self.temp_fc = Sequential(*([TemporalLayer(\n",
    "            mid_features, mid_features, mid_features), ] * num_temporal_layers))\n",
    "        self.out_norm = self.normalize(mid_features)\n",
    "        self.out_fc = Linear(mid_features, in_features)\n",
    "        self.t_proj = nn.Sequential(\n",
    "            Linear(mid_features, mid_features),\n",
    "            self.nonlinearity)\n",
    "        self.mid_features = mid_features\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t_emb = get_timestep_embedding(t, self.mid_features)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        out = self.in_fc(x)\n",
    "        out = self.temp_fc(out, t_emb=t_emb)\n",
    "        out = self.out_fc(self.out_norm(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24aed424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "model_mean_type = \"eps\"\n",
    "model_var_type = \"fixed-large\"\n",
    "loss_type = \"mse\"\n",
    "lat_dim = 50\n",
    "in_features = lat_dim\n",
    "out_features = 2 * in_features if model_var_type == \"learned\" else in_features\n",
    "mid_features = 256\n",
    "num_temporal_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56e3731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion parameters\n",
    "beta_schedule = \"linear\"\n",
    "beta_start, beta_end = 0.001, 0.2\n",
    "timesteps = 500\n",
    "betas = get_beta_schedule(beta_schedule, beta_start=beta_start, beta_end=beta_end, timesteps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a3c5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pigvae.models import GraphEncoder, GraphDecoder, Permuter \n",
    "from pigvae.models import BottleNeckEncoder, BottleNeckDecoder, PropertyPredictor\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, LayerNorm, Dropout\n",
    "from torch.nn.functional import relu, pad\n",
    "from pigvae.graph_transformer import Transformer, PositionalEncoding\n",
    "from pigvae.synthetic_graphs.data import DenseGraphBatch\n",
    "\n",
    "\n",
    "class GraphLDA(torch.nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        #self.vae = hparams[\"vae\"]\n",
    "        self.encoder = GraphEncoder(hparams)\n",
    "        self.bottle_neck_encoder = BottleNeckEncoder(hparams)\n",
    "        self.bottle_neck_decoder = BottleNeckDecoder(hparams)\n",
    "        self.property_predictor = PropertyPredictor(hparams)\n",
    "        self.permuter = Permuter(hparams)\n",
    "        self.decoder = GraphDecoder(hparams)\n",
    "\n",
    "        self.dense_fn = Denoiser(in_features, mid_features, num_temporal_layers)\n",
    "        self.diffusion = GaussianDiffusion(betas=betas, model_mean_type=model_mean_type, model_var_type=model_var_type, loss_type=loss_type)\n",
    "\n",
    "    def encode(self, node_features, edge_features, mask):\n",
    "\n",
    "        graph_emb, node_features = self.encoder(\n",
    "            node_features=node_features,\n",
    "            edge_features=edge_features,\n",
    "            mask=mask,\n",
    "        )\n",
    "        graph_emb, mu, logvar = self.bottle_neck_encoder(graph_emb)\n",
    "        return  graph_emb, mu, logvar, node_features\n",
    "\n",
    "    def decode(self, graph_emb, perm, mask=None):\n",
    "        props = self.property_predictor(graph_emb).squeeze()\n",
    "\n",
    "        \"\"\"\n",
    "        if mask is None:\n",
    "            num_nodes = torch.round(props * STD_NUM_NODES + MEAN_NUM_NODES).long()\n",
    "            mask = torch.arange(max(num_nodes)).type_as(num_nodes).unsqueeze(0) < num_nodes.unsqueeze(1)\n",
    "        \"\"\"\n",
    "        \n",
    "        graph_emb = self.bottle_neck_decoder(graph_emb)\n",
    "        node_logits, edge_logits = self.decoder(\n",
    "            graph_emb=graph_emb,\n",
    "            perm=perm,\n",
    "            mask=mask\n",
    "        )\n",
    "\n",
    "        return node_logits, edge_logits, props\n",
    "\n",
    "    def forward(self, node_features, edge_features, mask, training, tau):\n",
    "        graph_emb, mu, logvar, node_features = self.encode(node_features, edge_features, mask)\n",
    "        perm = self.permuter(node_features, mask=mask, hard=not training, tau=tau)\n",
    "\n",
    "        #diffusion process\n",
    "        z_0 = graph_emb\n",
    "        B = z_0.shape[0]\n",
    "        T = self.diffusion.timesteps\n",
    "        t = torch.randint(T, size=(B, ), dtype=torch.int64, device=device)\n",
    "        t_noise = torch.randn_like(graph_emb)\n",
    "        z_t = self.diffusion.q_sample(z_0, t, noise=t_noise)\n",
    "\n",
    "        #denoising\n",
    "        model_out = self.dense_fn(z_t, t) #model_out : 出力, t_noise : 予測するターゲット\n",
    "        \n",
    "        #deocde from noisy latent vector\n",
    "        graph_pred = self.decode(z_0, perm, mask)\n",
    "\n",
    "        return graph_pred, perm, graph_emb, mu, logvar, model_out, t_noise\n",
    "\n",
    "class BottleNeckEncoder(torch.nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.d_in = hparams[\"graph_encoder_hidden_dim\"]\n",
    "        self.d_out = hparams[\"emb_dim\"]\n",
    "        self.wu = Linear(self.d_in, self.d_out)\n",
    "        self.wv = Linear(self.d_in, self.d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.wu(relu(x))\n",
    "        logvar = self.wv(relu(x))\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f562e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphLDA(hparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f0ffb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphLDA(\n",
       "  (encoder): GraphEncoder(\n",
       "    (posiotional_embedding): PositionalEncoding()\n",
       "    (graph_transformer): Transformer(\n",
       "      (self_attn_layers): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pff_layers): ModuleList(\n",
       "        (0): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (12): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (13): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (14): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (15): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_in): Linear(in_features=83, out_features=256, bias=True)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (bottle_neck_encoder): BottleNeckEncoder(\n",
       "    (wu): Linear(in_features=256, out_features=50, bias=True)\n",
       "    (wv): Linear(in_features=256, out_features=50, bias=True)\n",
       "  )\n",
       "  (bottle_neck_decoder): BottleNeckDecoder(\n",
       "    (w): Linear(in_features=50, out_features=256, bias=True)\n",
       "  )\n",
       "  (property_predictor): PropertyPredictor(\n",
       "    (w_1): Linear(in_features=50, out_features=3, bias=True)\n",
       "    (w_2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (w_3): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (layer_norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (permuter): Permuter(\n",
       "    (scoring_fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): GraphDecoder(\n",
       "    (posiotional_embedding): PositionalEncoding()\n",
       "    (graph_transformer): Transformer(\n",
       "      (self_attn_layers): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pff_layers): ModuleList(\n",
       "        (0): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (12): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (13): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (14): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (15): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_in): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (node_fc_out): Linear(in_features=256, out_features=36, bias=True)\n",
       "    (edge_fc_out): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dense_fn): Denoiser(\n",
       "    (in_fc): Linear(in_features=50, out_features=256, bias=False)\n",
       "    (temp_fc): Sequential(\n",
       "      (0): TemporalLayer(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (enc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (skip): Identity()\n",
       "      )\n",
       "      (1): TemporalLayer(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (enc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (skip): Identity()\n",
       "      )\n",
       "      (2): TemporalLayer(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc2): Linear(in_features=256, out_features=256, bias=False)\n",
       "        (enc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (skip): Identity()\n",
       "      )\n",
       "    )\n",
       "    (out_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (out_fc): Linear(in_features=256, out_features=50, bias=True)\n",
       "    (t_proj): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.02, inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_dir = root + 'save_models/qm9/pig-beta-e3diffvae_with_trained-ae_models/'\n",
    "\n",
    "model = torch.load(load_model_dir + \"pigvae_best_model.pt\")\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523c96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "112ca6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import yaml\n",
    "#from easydict import EasyDict\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torch\n",
    "#import torch.utils.tensorboard\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edd37e",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0cb3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickes(load_dir):\n",
    "    with open(load_dir + \"node_features.pickle\", 'br') as fa:\n",
    "        node_features = pickle.load(fa)\n",
    "        fa.close\n",
    "          \n",
    "    with open(load_dir + \"edge_features.pickle\", 'br') as fb:\n",
    "        edge_features = pickle.load(fb)\n",
    "        fb.close\n",
    "          \n",
    "    with open(load_dir + \"masks.pickle\", 'br') as fc:\n",
    "        masks = pickle.load(fc)\n",
    "        fc.close\n",
    "\n",
    "    with open(load_dir + \"props.pickle\", 'br') as fd:\n",
    "        props = pickle.load(fd)\n",
    "        fd.close\n",
    "\n",
    "    with open(load_dir + \"all_targets.pickle\", 'br') as fe:\n",
    "        targets = pickle.load(fe)\n",
    "        fe.close\n",
    "        \n",
    "    return node_features, edge_features, masks, props, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5e9b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_to_embs(dataloader):\n",
    "    \n",
    "    z_mus = []\n",
    "    z_vars= []\n",
    "    t_list = []\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(dataloader):\n",
    "\n",
    "        batch_stds = []\n",
    "\n",
    "        node_features, edge_features, mask, props, targets = batch_data\n",
    "        node_features, edge_features, mask, props = node_features.to(device), edge_features.to(device), mask.to(device), props.to(device)\n",
    "\n",
    "        z, mu, logvar, node_features = model.encode(node_features, edge_features, mask)\n",
    "        batch_std = torch.exp(0.5 * logvar)\n",
    "    \n",
    "        for std in batch_std:\n",
    "            diag_matrix = torch.diag(std * std)\n",
    "            batch_stds.append(diag_matrix.cpu().detach().numpy())\n",
    "        \n",
    "        batch_stds = np.reshape(np.array(batch_stds), (-1, hparams[\"emb_dim\"], hparams[\"emb_dim\"]))\n",
    "\n",
    "        z_vars.extend(batch_stds)\n",
    "        z_mus.extend(mu.cpu().detach().numpy())\n",
    "        \n",
    "        t_list.extend(targets.cpu().detach().numpy())\n",
    "    \n",
    "        del z, mu, logvar, batch_std\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(batch_idx)\n",
    "        \n",
    "    return np.array(z_mus), np.array(z_vars), np.array(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38983f5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/torch_cuda/save_qm9-models/pig-beta-e3diffvae_with_trained-ae_models/samples_for_mcmc/\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "(100000, 50) (100000, 50, 50) (100000, 8)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "load_dir = root + \"dataset/eval_dataset/molecular_properties/qm9/e3graphs/\"\n",
    "sample_save_dir = load_model_dir + \"samples_for_mcmc/\"\n",
    "\n",
    "print(sample_save_dir)\n",
    "\n",
    "if not os.path.exists(sample_save_dir):\n",
    "    os.makedirs(sample_save_dir)\n",
    "\n",
    "node_features, edge_features, masks, props, targets = load_pickes(load_dir)\n",
    "dataset = torch.utils.data.TensorDataset(node_features, edge_features, masks, props, targets)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "z_mus, z_vars, ys = graphs_to_embs(dataloader)\n",
    "print(z_mus.shape, z_vars.shape, ys.shape)\n",
    "\n",
    "np.save(sample_save_dir + \"embs_mu.npy\", z_mus)\n",
    "np.save(sample_save_dir + \"embs_var.npy\", z_vars)\n",
    "np.save(sample_save_dir + \"all_targets.npy\", ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5ac54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
