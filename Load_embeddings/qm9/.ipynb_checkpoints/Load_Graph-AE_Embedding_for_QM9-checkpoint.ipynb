{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e74b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages']\n",
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages',\n",
      " '/torch_cuda/pigvae_all']\n",
      "['/opt/conda/lib/python310.zip',\n",
      " '/opt/conda/lib/python3.10',\n",
      " '/opt/conda/lib/python3.10/lib-dynload',\n",
      " '',\n",
      " '/opt/conda/lib/python3.10/site-packages',\n",
      " '/torch_cuda/pigvae_all',\n",
      " '/torch_cuda/ddpm-torch']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "root = '/'\n",
    "pprint.pprint(sys.path)\n",
    "\n",
    "import_path = root + 'pigvae_all'\n",
    "sys.path.append(import_path)\n",
    "pprint.pprint(sys.path)\n",
    "\n",
    "import_path2 = root + \"ddpm-torch\"\n",
    "sys.path.append(import_path2)\n",
    "pprint.pprint(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ca0bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "from networkx.algorithms.shortest_paths.dense import floyd_warshall_numpy\n",
    "\n",
    "from networkx.generators.random_graphs import *\n",
    "from networkx.generators.ego import ego_graph\n",
    "from networkx.generators.geometric import random_geometric_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b25d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_f = 36\n",
    "num_edge_f = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df7bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"vae\":True,\n",
    "    \"kld_loss_scale\":0.01,\n",
    "    \"perm_loss_scale\":0.1,\n",
    "    \"property_loss_scale\":0.5,\n",
    "    \"num_node_features\":num_node_f,\n",
    "    \"num_edge_features\":1+num_edge_f+1,\n",
    "    \"emb_dim\": 50,\n",
    "    'graph_encoder_hidden_dim': 256,\n",
    "    'graph_encoder_k_dim': 64,\n",
    "    'graph_encoder_v_dim': 64,\n",
    "    'graph_encoder_num_heads': 16,\n",
    "    'graph_encoder_ppf_hidden_dim': 512,\n",
    "    'graph_encoder_num_layers': 16,\n",
    "    'graph_decoder_hidden_dim': 256,\n",
    "    'graph_decoder_k_dim': 64,\n",
    "    'graph_decoder_v_dim': 64,\n",
    "    'graph_decoder_num_heads': 16,\n",
    "    'graph_decoder_ppf_hidden_dim': 512,\n",
    "    'graph_decoder_num_layers': 16,\n",
    "    \"graph_decoder_pos_emb_dim\": 64,\n",
    "    'property_predictor_hidden_dim': 3,\n",
    "    'num_properties': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcea199",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000e2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Linear, LayerNorm, Dropout\n",
    "from torch.nn.functional import relu, pad\n",
    "from pigvae.graph_transformer import Transformer, PositionalEncoding\n",
    "from pigvae.models import GraphAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f562e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphAE(hparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f0ffb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphAE(\n",
       "  (encoder): GraphEncoder(\n",
       "    (posiotional_embedding): PositionalEncoding()\n",
       "    (graph_transformer): Transformer(\n",
       "      (self_attn_layers): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pff_layers): ModuleList(\n",
       "        (0): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (12): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (13): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (14): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (15): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_in): Linear(in_features=83, out_features=256, bias=True)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (bottle_neck_encoder): BottleNeckEncoder(\n",
       "    (w): Linear(in_features=256, out_features=50, bias=True)\n",
       "  )\n",
       "  (bottle_neck_decoder): BottleNeckDecoder(\n",
       "    (w): Linear(in_features=50, out_features=256, bias=True)\n",
       "  )\n",
       "  (property_predictor): PropertyPredictor(\n",
       "    (w_1): Linear(in_features=50, out_features=3, bias=True)\n",
       "    (w_2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (w_3): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (layer_norm1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    (layer_norm2): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (permuter): Permuter(\n",
       "    (scoring_fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (decoder): GraphDecoder(\n",
       "    (posiotional_embedding): PositionalEncoding()\n",
       "    (graph_transformer): Transformer(\n",
       "      (self_attn_layers): ModuleList(\n",
       "        (0): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): SelfAttention(\n",
       "          (w_qs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_ks): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (w_vs): Linear(in_features=256, out_features=1024, bias=False)\n",
       "          (fc): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (attention): ScaledDotProductWithEdgeAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (pff_layers): ModuleList(\n",
       "        (0): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (12): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (13): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (14): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (15): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (w_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_in): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (node_fc_out): Linear(in_features=256, out_features=36, bias=True)\n",
       "    (edge_fc_out): Linear(in_features=256, out_features=8, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_dir = root + 'save_models/qm9/pig-e3ae_models/'\n",
    "\n",
    "model = torch.load(load_model_dir + \"pigvae_best_model.pt\")\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7e3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "112ca6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import yaml\n",
    "#from easydict import EasyDict\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torch\n",
    "#import torch.utils.tensorboard\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edd37e",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0cb3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickes(load_dir):\n",
    "    with open(load_dir + \"node_features.pickle\", 'br') as fa:\n",
    "        node_features = pickle.load(fa)\n",
    "        fa.close\n",
    "          \n",
    "    with open(load_dir + \"edge_features.pickle\", 'br') as fb:\n",
    "        edge_features = pickle.load(fb)\n",
    "        fb.close\n",
    "          \n",
    "    with open(load_dir + \"masks.pickle\", 'br') as fc:\n",
    "        masks = pickle.load(fc)\n",
    "        fc.close\n",
    "\n",
    "    with open(load_dir + \"props.pickle\", 'br') as fd:\n",
    "        props = pickle.load(fd)\n",
    "        fd.close\n",
    "\n",
    "    with open(load_dir + \"all_targets.pickle\", 'br') as fe:\n",
    "        targets = pickle.load(fe)\n",
    "        fe.close\n",
    "        \n",
    "    return node_features, edge_features, masks, props, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e9b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_to_embs(dataloader):\n",
    "    \n",
    "    z_mus = []\n",
    "    t_list = []\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(dataloader):\n",
    "\n",
    "        batch_stds = []\n",
    "        \n",
    "        if batch_idx % 100 == 0 :\n",
    "            print(batch_idx)\n",
    "\n",
    "        node_features, edge_features, mask, props, targets = batch_data\n",
    "        node_features, edge_features, mask, props = node_features.to(device), edge_features.to(device), mask.to(device), props.to(device)\n",
    "    \n",
    "        z, _, _, _ = model.encode(node_features, edge_features, mask)\n",
    "\n",
    "        z_mus.extend(z.cpu().detach().numpy())\n",
    "        t_list.extend(targets.cpu().detach().numpy())\n",
    "    \n",
    "        del z\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return np.array(z_mus), np.array(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38983f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/torch_cuda/save_qm9-models/pig-weighted-e3ae_models_50d/samples_for_mcmc/\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "(100000, 50) (100000, 8)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "load_dir = root + \"dataset/eval_dataset/molecular_properties/qm9/e3graphs/\"\n",
    "sample_save_dir = load_model_dir + \"samples_for_mcmc/\"\n",
    "\n",
    "print(sample_save_dir)\n",
    "\n",
    "if not os.path.exists(sample_save_dir):\n",
    "    os.makedirs(sample_save_dir)\n",
    "\n",
    "node_features, edge_features, masks, props, targets = load_pickes(load_dir)\n",
    "dataset = torch.utils.data.TensorDataset(node_features, edge_features, masks, props, targets)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "z_mus, ys = graphs_to_embs(dataloader)\n",
    "print(z_mus.shape, ys.shape)\n",
    "\n",
    "np.save(sample_save_dir + \"embs_mu.npy\", z_mus)\n",
    "np.save(sample_save_dir + \"all_targets.npy\", ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a57e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
